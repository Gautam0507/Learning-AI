{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Grid_World\n",
    "import gymnasium as gym\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_size = 4\n",
    "env = Grid_World(size = env_size, type= \"fixed\", render_mode= \"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x144ace88d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state, target = env.reset()\n",
    "frame = env.render()\n",
    "plt.axis('off')\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]]\n"
     ]
    }
   ],
   "source": [
    "policy_probablities = np.full((env_size, env_size, 4), 0.25)\n",
    "print(policy_probablities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    policy_probs = policy_probablities[state]\n",
    "    return np.random.choice(4, p = policy_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "state_values = np.zeros(shape = (env_size, env_size))\n",
    "print(state_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the policy iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy_probablities, state_values, theta = 1e-6, gamma= 0.99) ->  None:\n",
    "    delta = float(\"inf\")\n",
    "\n",
    "    while delta > theta: \n",
    "        delta = 0\n",
    "        for row in range(env_size):\n",
    "            for col in range(env_size):\n",
    "                state = (row,col)\n",
    "                old_value = state_values[state]\n",
    "                new_value = 0.\n",
    "                action_probablities = policy_probablities[state]\n",
    "\n",
    "                for action, prob in enumerate(action_probablities):\n",
    "                    next_state, reward, _, _ = env.simulate_step(state, action)\n",
    "                    new_value += prob * (reward + gamma * state_values[next_state])\n",
    "\n",
    "                state_values[state] = new_value\n",
    "                delta = max(delta, abs(old_value - new_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(policy_probablities, state_values, gamma = 0.99):\n",
    "    policy_stable = True\n",
    "\n",
    "    for row in range(env_size): \n",
    "        for col in range(env_size):\n",
    "            state = (row, col)\n",
    "            old_action = policy_probablities[state].argmax()\n",
    "\n",
    "            new_action = None\n",
    "            max_qsa = float(\"-inf\")\n",
    "\n",
    "            for action in range(4):\n",
    "                next_state, reward, _, _ = env.simulate_step(state, action)\n",
    "                qsa = reward + gamma * state_values[next_state]\n",
    "\n",
    "                if qsa > max_qsa: \n",
    "                    new_action = action\n",
    "                    max_qsa = qsa\n",
    "\n",
    "        action_probs = np.zeros(4)\n",
    "        action_probs[new_action] = 1.\n",
    "        policy_probablities[state[0], state[1]] = action_probs\n",
    "\n",
    "        if new_action != old_action:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(policy_probablities, state_values, theta = 1e-6, gamma = 0.99):\n",
    "    policy_stable = False\n",
    "\n",
    "    while not policy_stable:\n",
    "        policy_evaluation(policy_probablities, state_values, theta, gamma)\n",
    "        policy_stable = policy_improvement(policy_probablities, state_values, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m policy_iteration(policy_probablities, state_values)\n",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[1;34m(policy_probablities, state_values, theta, gamma)\u001b[0m\n\u001b[0;32m      2\u001b[0m policy_stable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m policy_stable:\n\u001b[1;32m----> 5\u001b[0m     policy_evaluation(policy_probablities, state_values, theta, gamma)\n\u001b[0;32m      6\u001b[0m     policy_stable \u001b[39m=\u001b[39m policy_improvement(policy_probablities, state_values, gamma)\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mpolicy_evaluation\u001b[1;34m(policy_probablities, state_values, theta, gamma)\u001b[0m\n\u001b[0;32m     14\u001b[0m     next_state, reward, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39msimulate_step(state, action)\n\u001b[0;32m     15\u001b[0m     new_value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prob \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m state_values[next_state])\n\u001b[1;32m---> 17\u001b[0m state_values[state] \u001b[39m=\u001b[39m new_value\n\u001b[0;32m     18\u001b[0m delta \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(delta, \u001b[39mabs\u001b[39m(old_value \u001b[39m-\u001b[39m new_value))\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "policy_iteration(policy_probablities, state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
