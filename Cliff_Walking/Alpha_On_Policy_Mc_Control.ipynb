{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gymnasium as gym \n",
    "import matplotlib.pyplot as plt \n",
    "from statistics import mean\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "\n",
    "from env import CliffWalking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalking(render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Q(s|a) value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_values = np.random.rand(48,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the value with state (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03897977, 0.4108894 , 0.4185447 , 0.0213924 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, epsilon=0.2):\n",
    "    action_probablities = action_values[state]\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(action_probablities)\n",
    "    return action\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_mc_cotrol(policy: callable, action_values, episodes:  int = 10000, gamma: int = 0.99, epsilon: int = 0.2, alpha:int = 0.2) -> None:\n",
    "    sa_returns = np.empty(shape=(48, 4), dtype=object)\n",
    "\n",
    "    for episode in range(episodes + 1):\n",
    "        trajectory = []\n",
    "        state = env.reset()\n",
    "        done, terminated = False, False\n",
    "        while not done or terminated:\n",
    "            action = policy(state, epsilon)\n",
    "            next_state, reward, done, terminated = env.step(action)\n",
    "            trajectory.append([state, action, reward])\n",
    "            state = next_state\n",
    "\n",
    "        print(episode)\n",
    "        G = 0\n",
    "\n",
    "        for state_t, action_t, reward_t in reversed(trajectory):\n",
    "            G = reward_t + gamma * G\n",
    "\n",
    "            qsa = action_values[state_t][action_t]\n",
    "            action_values[state_t][action_t] += alpha * (G - qsa)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m on_policy_mc_cotrol(policy, action_values, episodes \u001b[39m=\u001b[39;49m \u001b[39m10000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mon_policy_mc_cotrol\u001b[1;34m(policy, action_values, episodes, gamma, epsilon, alpha)\u001b[0m\n\u001b[0;32m      7\u001b[0m done, terminated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done \u001b[39mor\u001b[39;00m terminated:\n\u001b[1;32m----> 9\u001b[0m     action \u001b[39m=\u001b[39m policy(state, epsilon)\n\u001b[0;32m     10\u001b[0m     next_state, reward, done, terminated \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     11\u001b[0m     trajectory\u001b[39m.\u001b[39mappend([state, action, reward])\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mpolicy\u001b[1;34m(state, epsilon)\u001b[0m\n\u001b[0;32m      4\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m      5\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(action_probablities)\n\u001b[0;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\GAUTAM\\Desktop\\aiml\\beginner reinforcement learning\\rl practice\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\GAUTAM\\Desktop\\aiml\\beginner reinforcement learning\\rl practice\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "on_policy_mc_cotrol(policy, action_values, episodes = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.50865675e+002 -3.94206396e+002 -1.26394075e+002 -3.34120662e+002]\n",
      " [-2.50442721e+002 -1.62737158e+002 -3.78139280e+002 -2.62752549e+002]\n",
      " [-9.44710362e+001 -1.73287831e+001 -8.35544787e+001 -1.44194744e+002]\n",
      " [-1.14649370e+002 -2.32470649e+002 -1.59334513e+001 -6.39802135e+001]\n",
      " [-2.54423631e+001 -2.05118946e+002 -9.92854032e+001 -5.28424876e+001]\n",
      " [-7.49247151e+001 -1.56102628e+001 -1.74909545e+002 -1.38829839e+002]\n",
      " [-1.00155207e+002 -1.14160002e+002 -7.45835386e+001 -1.52476433e+001]\n",
      " [-3.16885914e+001 -2.03228893e+001 -7.87273706e+001 -1.71729871e+002]\n",
      " [-2.45184230e+001 -5.64649500e+000 -2.10303527e+001 -2.13332022e+001]\n",
      " [-2.59573867e+001 -1.69729956e+001 -4.37054803e+000 -1.31759063e+001]\n",
      " [-9.27074171e+000 -4.57286900e+000 -2.28643256e+001 -7.59049970e+000]\n",
      " [-3.09170984e+001 -6.30028571e+000 -2.85479266e+000 -2.43745815e+001]\n",
      " [-2.70923235e+002 -2.32047631e+002 -1.82793098e+002 -2.23669452e+001]\n",
      " [-3.51885850e+002 -7.72389727e+001 -1.02602455e+003 -3.09639690e+002]\n",
      " [-1.92724107e+001 -1.14602571e+002 -2.35816594e+002 -2.29329443e+002]\n",
      " [-1.01666715e+002 -4.91018276e+001 -1.85225125e+002 -1.42460663e+002]\n",
      " [-4.76000863e+001 -3.19909202e+001 -2.68920305e+002 -9.20555824e+001]\n",
      " [-1.70430851e+002 -1.02400433e+001 -1.71148086e+002 -1.73413751e+002]\n",
      " [-6.65153101e+001 -8.68465022e+000 -6.87779902e+001 -1.11096467e+002]\n",
      " [-1.05920415e+002 -7.35955303e+000 -5.95172392e+001 -2.83600837e+001]\n",
      " [-6.85633665e+000 -2.48654005e+001 -3.82984921e+001 -4.39548185e+001]\n",
      " [-5.12207213e+001 -3.17308509e+000 -5.92472734e+001 -6.06193989e+001]\n",
      " [-6.43559540e+000 -2.17819595e+000 -1.15754986e+001 -7.86504954e+000]\n",
      " [-4.35501150e+000 -2.36171703e+000 -1.01396745e+000 -4.11949684e+000]\n",
      " [-1.96986681e+002 -5.40368474e+001 -3.35614565e+002 -2.91826266e+002]\n",
      " [-5.91551752e+002 -5.35176653e+001 -3.34437295e+002 -1.49414637e+002]\n",
      " [-3.91882928e+001 -3.07242747e+002 -4.34785105e+002 -3.89164916e+002]\n",
      " [-3.00367987e+002 -9.94482720e+001 -4.93441579e+002 -4.62133525e+002]\n",
      " [-7.70081907e+001 -1.92614097e+002 -2.85534962e+002 -2.57049342e+002]\n",
      " [-2.86239129e+002 -8.05313199e+001 -2.41376322e+002 -2.85654395e+002]\n",
      " [-1.33225635e+002 -2.61526534e+001 -2.56897111e+002 -1.39048279e+002]\n",
      " [-3.06014240e+001 -8.08284115e+001 -1.54024151e+002 -1.14989197e+002]\n",
      " [-6.56754704e+001 -7.50954738e+001 -1.49850687e+002 -1.04868487e+002]\n",
      " [-5.41671434e+000 -7.17743405e+001 -2.14954752e+002 -9.52293923e+001]\n",
      " [-1.01556695e+001 -1.06031858e+000 -1.10250892e+002 -5.06305244e+001]\n",
      " [-2.88262152e+000 -1.74466996e+000  9.88131292e-324 -2.96902680e+000]\n",
      " [-5.41627778e+001 -1.11267576e+003 -1.16732644e+003 -2.75191623e+002]\n",
      " [-2.09855092e+002 -8.79677042e+002 -2.90626076e+003 -6.34192409e+002]\n",
      " [-3.36459661e+002 -9.26536888e+002 -6.39674100e+002 -7.01401092e+002]\n",
      " [-3.69991095e+002 -2.94963913e+002 -8.72837025e+002 -8.53819068e+002]\n",
      " [-1.03639177e+003 -1.70475915e+002 -4.94938996e+002 -5.88281741e+002]\n",
      " [-4.81240456e+001 -4.78803026e+002 -3.37338196e+002 -4.42695789e+002]\n",
      " [-2.95804893e+002 -1.48203041e+002 -2.92103091e+002 -3.67843581e+002]\n",
      " [-4.30756025e+001 -2.73890045e+002 -4.52826037e+002 -3.38197101e+002]\n",
      " [-3.05112679e+001 -2.74131762e+002 -3.20075617e+002 -3.26752091e+002]\n",
      " [-1.66614888e+002 -1.16850630e+002 -2.03874380e+002 -3.47113664e+002]\n",
      " [-2.20513637e+001  1.01580922e-035 -1.46774439e+002 -2.86669350e+002]\n",
      " [ 4.95183772e-001  6.27439981e-001  6.69632115e-001  1.18252281e-001]]\n"
     ]
    }
   ],
   "source": [
    "print(action_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The test agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(policy, episodes=1, epsilon=0.2):\n",
    "    env.pygame_init()\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done, terminated = False, False\n",
    "        while not (done or terminated):\n",
    "            action = policy(state, epsilon)\n",
    "            next_state, reward, done, terminated = env.step(action)\n",
    "            frame = env.render()\n",
    "            state = next_state\n",
    "        print(episode+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test_agent(policy, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
